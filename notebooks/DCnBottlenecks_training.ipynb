{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615036ee-e462-4a07-bdd2-db1d6f342c21",
   "metadata": {},
   "source": [
    "# Train DCnBottlenecks model\n",
    "aim to find the minimum number of linear bottlenecks to account for weight drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f46d3225-d83e-4954-84e8-1dc88b52ac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "repo_path = r'C:\\Users\\mengz\\Box\\Li_Lab\\DropConnect\\METHODS\\dropnet'\n",
    "sys.path.append(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "001eded5-a726-4f3e-8c08-20d24de5011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import importlib\n",
    "import inspect\n",
    "import warnings\n",
    "import json\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import pickle as pkl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "\n",
    "from recorder import Plot\n",
    "from training_utils import train, validate, save_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a4d7b-f53e-4dee-9f0a-4dbbf6ff7ef8",
   "metadata": {},
   "source": [
    "## Import training argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0963555f-9bd8-4e6d-a448-d524a77a7550",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = r'C:\\Users\\mengz\\Box\\Li_Lab\\DropConnect\\METHODS\\dropnet'\n",
    "param_folder_path = f'{repo_path}\\config'\n",
    "file_name = r'\\DCnBottlenecks'\n",
    "param_file = f'{param_folder_path}{file_name}.json'\n",
    "with open(param_file, 'r') as f:\n",
    "    args_from_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a46b4ca7-0536-4565-8910-f8330dde80b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(self, config_dict):\n",
    "        for key, value in config_dict.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args = Arguments(args_from_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd5bf797-e1d2-4eae-bf60-50305e5305bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.gpu = None\n",
    "args.milestones = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a0e21-994e-4944-b9a7-b1dc4303555f",
   "metadata": {},
   "source": [
    "## Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df792b56-b9ff-45c9-83cf-5b1642e5c36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model models.DCnBottlenecks successfully imported\n"
     ]
    }
   ],
   "source": [
    "model_path = 'models'\n",
    "module = importlib.import_module(f'{model_path}.{args.arch}')\n",
    "model_class = None\n",
    "\n",
    "for name, obj in inspect.getmembers(module):\n",
    "    if inspect.isclass(obj):\n",
    "        if model_path + '.' + name == module.__name__:\n",
    "            print(f'Model {module.__name__} successfully imported')\n",
    "            model_class = obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de75d6b-0f1e-4b0f-a429-8230a73f9a1e",
   "metadata": {},
   "source": [
    "## Model configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "851445bb-cb61-4113-8504-311983d9e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = [\n",
    "                 # {'drop_probs': 0.3,\n",
    "                 #  # [[output_dimension_1, stride_1, expand_ratio_1], ....]\n",
    "                 #  'bottleneck_cfg': [[8, 1, 6],]},\n",
    "                 {'drop_probs': 0.3,\n",
    "                  'bottleneck_cfg': [[8, 1, 6], [8, 1, 6]]},\n",
    "                 # {'drop_probs': 0.3,\n",
    "                 #  'bottleneck_cfg': [[8, 1, 6], [8, 1, 6], [8, 1, 6]]},\n",
    "                 # {'drop_probs': 0.3,\n",
    "                 #  'bottleneck_cfg': [[8, 1, 6], [8, 1, 6], [8, 1, 6], [8, 1, 6]]},\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3145182-6797-481f-a312-bf9630bc690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{checkpoints_path}\\model_config.pkl', 'wb') as f_cfg:\n",
    "    pkl.dump(cfg, f_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4125d4-9651-425d-973d-cfe61c498091",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e604e3cc-2de8-4f86-ba2a-4a29b8265558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created\n",
      "using CPU, this will be slow\n",
      "GPU and model setup done\n",
      "Data ready\n",
      "Begin training\n",
      "Epoch: [0][  1/235]\tTime  6.929 ( 6.929)\tData  6.538 ( 6.538)\tLoss 2.6018e+00 (2.6018e+00)\tAcc@1   8.98 (  8.98)\tAcc@5  52.34 ( 52.34)\n",
      "Epoch: [0][ 51/235]\tTime  0.341 ( 0.487)\tData  0.000 ( 0.129)\tLoss 4.6221e-01 (7.5760e-01)\tAcc@1  85.94 ( 77.77)\tAcc@5  98.83 ( 96.49)\n",
      "Epoch: [0][101/235]\tTime  0.335 ( 0.421)\tData  0.000 ( 0.065)\tLoss 3.1045e-01 (5.8362e-01)\tAcc@1  90.23 ( 83.06)\tAcc@5  99.61 ( 97.91)\n",
      "Epoch: [0][151/235]\tTime  0.334 ( 0.400)\tData  0.000 ( 0.044)\tLoss 2.3512e-01 (4.9694e-01)\tAcc@1  93.36 ( 85.45)\tAcc@5 100.00 ( 98.45)\n",
      "Epoch: [0][201/235]\tTime  0.340 ( 0.390)\tData  0.001 ( 0.033)\tLoss 3.0416e-01 (4.5502e-01)\tAcc@1  91.02 ( 86.62)\tAcc@5  99.22 ( 98.71)\n",
      "Test: [ 1/79]\tTime  8.781 ( 8.781)\tLoss 4.8954e+00 (4.8954e+00)\tAcc@1  27.34 ( 27.34)\tAcc@5  89.84 ( 89.84)\n",
      "Test: [51/79]\tTime  0.046 ( 0.228)\tLoss 3.8919e+00 (4.3738e+00)\tAcc@1  42.97 ( 34.01)\tAcc@5  89.06 ( 88.42)\n",
      "---- save figure learning_curve_path into C:\\Users\\mengz\\Box\\Li_Lab\\DropConnect\\METHODS\\dropnet\\learning_curve\\DC2Bottlenecks_dr0.3_learning_curve.png\n",
      "Epoch: [1][  1/235]\tTime  7.929 ( 7.929)\tData  7.436 ( 7.436)\tLoss 2.5113e-01 (2.5113e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.22 ( 99.22)\n",
      "Epoch: [1][ 51/235]\tTime  0.421 ( 0.545)\tData  0.000 ( 0.150)\tLoss 3.1260e-01 (2.3568e-01)\tAcc@1  92.19 ( 92.85)\tAcc@5 100.00 ( 99.74)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 138\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBegin training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mstart_epoch, args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# train for one epoch\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     t_acc1, t_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# evaluate on validation set\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     v_acc1, v_loss \u001b[38;5;241m=\u001b[39m validate(val_loader, model, criterion, args)\n",
      "File \u001b[1;32m~\\Box\\Li_Lab\\DropConnect\\METHODS\\dropnet\\training_utils.py:43\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer, epoch, device, args)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# compute gradient and do SGD step\u001b[39;00m\n\u001b[0;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 43\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# measure elapsed time\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dropnet_env\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dropnet_env\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for cfg in model_configs:\n",
    "    best_acc1 = 0\n",
    "    \n",
    "    # -------------------- create model --------------------\n",
    "    cur_bottleneck_cfg = cfg.get('bottleneck_cfg')\n",
    "    cur_drop_probs = cfg.get('drop_probs')\n",
    "    model = model_class(cur_bottleneck_cfg, drop_probs = cur_drop_probs)\n",
    "    \n",
    "    learning_curve_path = os.path.join(f'{repo_path}\\learning_curve', f'{model.name}_dr{cur_drop_probs}_learning_curve.png')\n",
    "    checkpoints_path = os.path.join(f'{repo_path}\\checkpoints', f'{model.name}_dr{cur_drop_probs}')\n",
    "\n",
    "    print('Model created')\n",
    "    \n",
    "    # -------------------- GPU settings --------------------\n",
    "    if args.gpu is not None:\n",
    "        warnings.warn('You have chosen a specific GPU. This will completely '\n",
    "                      'disable data parallelism.')\n",
    "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "\n",
    "    if not torch.cuda.is_available() and not torch.backends.mps.is_available():\n",
    "        print('using CPU, this will be slow')\n",
    "    elif args.gpu is not None and torch.cuda.is_available():\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        model = model.cuda(args.gpu)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        # if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):\n",
    "        #     model.features = torch.nn.DataParallel(model.features)\n",
    "        #     model.cuda()\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        if args.gpu:\n",
    "            device = torch.device('cuda:{}'.format(args.gpu))\n",
    "        else:\n",
    "            device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        \n",
    "    # define loss function (criterion), optimizer, and learning rate scheduler\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    if args.optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                            momentum=args.momentum,\n",
    "                            weight_decay=args.weight_decay)\n",
    "    elif args.optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "   \n",
    "    if args.milestones is None:\n",
    "        scheduler = StepLR(optimizer, step_size=args.lr_step, gamma=args.gamma)\n",
    "    else:\n",
    "        scheduler = MultiStepLR(optimizer, milestones=args.milestones,\n",
    "                                gamma=args.gamma, last_epoch=args.start_epoch - 1)\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            if args.gpu is None:\n",
    "                checkpoint = torch.load(args.resume)\n",
    "            elif torch.cuda.is_available():\n",
    "                # Map model to be loaded to specified single gpu.\n",
    "                loc = 'cuda:{}'.format(args.gpu)\n",
    "                checkpoint = torch.load(args.resume, map_location=loc)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_acc1 = checkpoint['best_acc1']\n",
    "            if args.gpu is not None:\n",
    "                # best_acc1 may be from a checkpoint from a different GPU\n",
    "                best_acc1 = best_acc1.to(args.gpu)\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "    print('GPU and model setup done')\n",
    "    \n",
    "    # -------------------- Data loading --------------------\n",
    "    if args.dataset == 'mnist':\n",
    "        train_dataset = datasets.MNIST('./data', train=True,\n",
    "            transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]), download=True)\n",
    "\n",
    "        val_dataset = datasets.MNIST('./data', train=False,\n",
    "            transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]), download=True)\n",
    "    elif args.dataset == 'cifar10':\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]), download=True)\n",
    "\n",
    "        val_dataset = datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    else:\n",
    "        raise Exception(\"Dataset not supported\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size, shuffle=True,\n",
    "        num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=128, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    print('Data ready')\n",
    "    \n",
    "    # -------------------- Model training --------------------\n",
    "    if args.evaluate:\n",
    "        validate(val_loader, model, criterion, args)\n",
    "        continue\n",
    "    \n",
    "    plt = Plot()\n",
    "\n",
    "    print('Begin training')\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        # train for one epoch\n",
    "        t_acc1, t_loss = train(train_loader, model, criterion, optimizer, epoch, device, args)\n",
    "    \n",
    "        # evaluate on validation set\n",
    "        v_acc1, v_loss = validate(val_loader, model, criterion, args)\n",
    "    \n",
    "        plt.record(t_acc1.item(), t_loss, v_acc1.item(), v_loss)\n",
    "        plt.plot(learning_curve_path)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # remember best acc@1 and save checkpoint\n",
    "        is_best = v_acc1 > best_acc1\n",
    "        best_acc1 = max(v_acc1, best_acc1)\n",
    "    \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': args.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "            'scheduler' : scheduler.state_dict()\n",
    "        }, is_best, checkpoints_path)\n",
    "\n",
    "    # -------------------- Save model config --------------------\n",
    "    try:\n",
    "        with open(f'{checkpoints_path}\\model_config.pkl') as f_cfg:\n",
    "            pkl.dump(cfg, f_cfg)\n",
    "    except Exception:\n",
    "        print('Model config pickle dump failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d06841-4bd7-4a3a-a015-da1679521457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
